{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Wiki Data Dump\n",
    "# https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2\n",
    "\n",
    "# Amazon Product Reviews\n",
    "# https://snap.stanford.edu/data/web-Amazon.htmls\n",
    "\n",
    "# Book Data\n",
    "# English Books\n",
    "# http://opus.lingfil.uu.se/download.php?f=Books/en.tar.gz\n",
    "# French Books\n",
    "# http://opus.lingfil.uu.se/download.php?f=Books/fr.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# English movie subtitles\n",
    "# http://opus.lingfil.uu.se/download.php?f=OpenSubtitles2016/mono/OpenSubtitles2016.raw.en.gz\n",
    "\n",
    "# French movie subtitles\n",
    "# http://opus.lingfil.uu.se/download.php?f=OpenSubtitles2016/mono/OpenSubtitles2016.raw.fr.gz   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import collections\n",
    "import urllib.request\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Download data\n",
    "\n",
    "# Download the dataset if it's not already there: this may take a minute as it is 75MB\n",
    "if not os.path.isfile(\"data/english_subtitles.gz\"):\n",
    "    urllib.request.urlretrieve(\"http://opus.lingfil.uu.se/download.php?f=OpenSubtitles2016/mono/OpenSubtitles2016.raw.en.gz\", filename=\"data/english_subtitles.gz\")\n",
    "\n",
    "if not os.path.isfile(\"data/french_subtitles.gz\"):\n",
    "    urllib.request.urlretrieve(\"http://opus.lingfil.uu.se/download.php?f=OpenSubtitles2016/mono/OpenSubtitles2016.raw.fr.gz\", filename=\"data/french_subtitles.gz\")\n",
    "    \n",
    "with gzip.open('english_subtitles.gz', 'rb') as f:\n",
    "    english_content = f.read()\n",
    "    \n",
    "with gzip.open('french_subtitles.gz', 'rb') as f:\n",
    "    french_content = f.read()\n",
    "\n",
    "print(\"data downloaded\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Build LSTM graph\n",
    "\n",
    "# vocab_size = 100000\n",
    "# num_layers = 8\n",
    "# num_steps = 2000\n",
    "# hidden_size = 500\n",
    "\n",
    "\n",
    "# def length(sequence):\n",
    "#     used = tf.sign(tf.reduce_max(tf.abs(sequence), reduction_indices=2))\n",
    "#     length = tf.reduce_sum(used, reduction_indices=1)\n",
    "#     length = tf.cast(length, tf.int32)\n",
    "#     return length\n",
    "\n",
    "# x = tf.placeholder(\"float16\", shape=[\n",
    "#                    None, num_steps, vocab_size], name=\"x_placeholder\")\n",
    "# y = tf.placeholder(\"float16\", shape=[None, num_outputs], name=\"y_placeholder\")\n",
    "\n",
    "# weights = tf.Variable(tf.truncated_normal(\n",
    "#     [hidden_size, num_outputs], stddev=0.05, dtype=tf.float16))\n",
    "# bias = tf.Variable(tf.constant(.1, shape=[num_outputs], dtype=tf.float16))\n",
    "\n",
    "# lstm = tf.contrib.rnn.BasicLSTMCell(\n",
    "#     hidden_size, forget_bias=0.0, state_is_tuple=True)\n",
    "# stacked_lstm = tf.contrib.rnn.MultiRNNCell(\n",
    "#     [lstm] * num_layers, state_is_tuple=True)\n",
    "\n",
    "# outputs, state = tf.contrib.legacy_seq2seq.embedding_rnn_seq2seq(\n",
    "#     encoder_inputs=x, decoder_inputs=x, cell=stacked_lstm, num_encoder_symbols=vocab_size, num_decoder_symbols=vocab_size, embedding_size=1000, output_projection=None, feed_previous=False,dtype=tf.float16)\n",
    "\n",
    "# # outputs = tf.transpose(outputs, [1,0,2])\n",
    "# # last = tf.gather(outputs, num_steps - 1)\n",
    "# # y_pred = tf.nn.softmax(tf.matmul(last, weights) + bias)\n",
    "# outputs = tf.reduce_mean(outputs, 1)\n",
    "# y_pred = tf.nn.softmax(tf.matmul(outputs, weights) + bias)\n",
    "\n",
    "# cross_entropy = tf.reduce_mean(\n",
    "#     tf.nn.softmax_cross_entropy_with_logits(logits=y_pred, labels=y))\n",
    "# optimizer = tf.train.AdamOptimizer(\n",
    "#     learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "# correct_pred = tf.equal(tf.argmax(y_pred, 1), tf.argmax(y, 1))\n",
    "# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
